#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”¨æˆ·æ„å›¾åˆ†æå’Œæ£€ç´¢è¯æ„å»ºæ¨¡å— v2.0
åŸºäºAIå¤§æ¨¡å‹åˆ†æç”¨æˆ·è¾“å…¥ï¼Œç”ŸæˆPubMedæ£€ç´¢è¯å’Œç­›é€‰æ¡ä»¶
ä¼˜åŒ–ç‰¹æ€§ï¼šæ™ºèƒ½ç¼“å­˜ã€å¼‚æ­¥å¤„ç†ã€å¢å¼ºé”™è¯¯å¤„ç†ã€é…ç½®ç®¡ç†ä¼˜åŒ–
"""

import json
import os
import re
import time
import asyncio
import hashlib
import threading
from typing import Dict, List, Optional, Tuple, Any, TYPE_CHECKING
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from src.ai_client import AIClient, ConfigManager, ChatMessage
from src.prompts_manager import PromptsManager
from dataclasses import dataclass, asdict


@dataclass
class AIModelConfig:
    """AIæ¨¡å‹é…ç½®ç¼“å­˜"""
    config_name: str
    model_id: str
    parameters: Dict
    
    def to_dict(self):
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict):
        return cls(**data)


class IntentAnalysisCache:
    """æ„å›¾åˆ†æç»“æœç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_size: int = 500, ttl: int = 3600):
        self.cache_size = cache_size
        self.ttl = ttl
        self.cache = {}
        self.access_times = {}
        self.lock = threading.Lock()
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
    
    def _generate_cache_key(self, user_input: str, model_id: str, parameters: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{user_input}:{model_id}:{json.dumps(parameters, sort_keys=True)}"
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def get(self, user_input: str, model_id: str, parameters: Dict) -> Optional['SearchCriteria']:
        """è·å–ç¼“å­˜çš„æ„å›¾åˆ†æç»“æœ"""
        cache_key = self._generate_cache_key(user_input, model_id, parameters)
        
        with self.lock:
            if cache_key in self.cache:
                cache_data = self.cache[cache_key]
                if time.time() - cache_data['timestamp'] < self.ttl:
                    self.access_times[cache_key] = time.time()
                    self.stats['hits'] += 1
                    return cache_data['criteria']
                else:
                    # æ¸…é™¤è¿‡æœŸç¼“å­˜
                    del self.cache[cache_key]
                    if cache_key in self.access_times:
                        del self.access_times[cache_key]
                    self.stats['evictions'] += 1
        
        self.stats['misses'] += 1
        return None
    
    def put(self, user_input: str, model_id: str, parameters: Dict, criteria: 'SearchCriteria'):
        """ç¼“å­˜æ„å›¾åˆ†æç»“æœ"""
        cache_key = self._generate_cache_key(user_input, model_id, parameters)
        
        with self.lock:
            # LRUç¼“å­˜æ¸…ç†
            if len(self.cache) >= self.cache_size:
                oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
                del self.cache[oldest_key]
                del self.access_times[oldest_key]
                self.stats['evictions'] += 1
            
            self.cache[cache_key] = {
                'criteria': criteria,
                'timestamp': time.time(),
                'user_input': user_input,
                'model_id': model_id,
                'parameters': parameters
            }
            self.access_times[cache_key] = time.time()
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.access_times.clear()
            self.stats = {'hits': 0, 'misses': 0, 'evictions': 0}
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.stats['hits'] + self.stats['misses']
            hit_rate = self.stats['hits'] / total_requests if total_requests > 0 else 0
            
            return {
                'cache_size': len(self.cache),
                'max_cache_size': self.cache_size,
                'hit_rate': hit_rate,
                'hits': self.stats['hits'],
                'misses': self.stats['misses'],
                'evictions': self.stats['evictions']
            }


class ConfigManagerPool:
    """é…ç½®ç®¡ç†å™¨æ±  - é¿å…é‡å¤åˆå§‹åŒ–"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.config_managers = {}
            self.ai_clients = {}
            self.adapters = {}
            self.lock = threading.Lock()
            self._initialized = True
    
    def get_config_manager(self, config_file: str = "ai_config.yaml") -> ConfigManager:
        """è·å–é…ç½®ç®¡ç†å™¨å®ä¾‹"""
        with self.lock:
            if config_file not in self.config_managers:
                self.config_managers[config_file] = ConfigManager(config_file)
            return self.config_managers[config_file]
    
    def get_ai_client(self, config_file: str = "ai_config.yaml", 
                     enable_cache: bool = True, enable_retry: bool = True) -> AIClient:
        """è·å–AIå®¢æˆ·ç«¯å®ä¾‹"""
        key = f"{config_file}:{enable_cache}:{enable_retry}"
        with self.lock:
            if key not in self.ai_clients:
                self.ai_clients[key] = AIClient(enable_cache, enable_retry)
            return self.ai_clients[key]
    
    def clear_all(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„å®ä¾‹"""
        with self.lock:
            self.config_managers.clear()
            self.ai_clients.clear()
            self.adapters.clear()


@dataclass
class SearchCriteria:
    """æœç´¢æ¡ä»¶æ•°æ®ç±»"""
    query: str  # PubMedæ£€ç´¢è¯
    year_start: Optional[int] = None  # èµ·å§‹å¹´ä»½
    year_end: Optional[int] = None  # ç»“æŸå¹´ä»½
    min_if: Optional[float] = None  # æœ€å°å½±å“å› å­
    max_if: Optional[float] = None  # æœ€å¤§å½±å“å› å­
    cas_zones: List[int] = None  # ä¸­ç§‘é™¢åˆ†åŒºé™åˆ¶ [1,2,3,4]
    jcr_quartiles: List[str] = None  # JCRåˆ†åŒºé™åˆ¶ ["Q1","Q2","Q3","Q4"]
    keywords: List[str] = None  # å…³é”®è¯è¿‡æ»¤
    
    def __post_init__(self):
        if self.cas_zones is None:
            self.cas_zones = []
        if self.jcr_quartiles is None:
            self.jcr_quartiles = []
        if self.keywords is None:
            self.keywords = []


class IntentAnalyzer:
    """ç”¨æˆ·æ„å›¾åˆ†æå™¨ v2.0 - ä¼˜åŒ–ç‰ˆ"""
    
    CONFIG_CACHE_FILE = "ai_model_cache.json"
    
    def __init__(self, config_name: str = None, interactive: bool = True, 
                 enable_cache: bool = True, enable_async: bool = True):
        """
        åˆå§‹åŒ–æ„å›¾åˆ†æå™¨
        
        Args:
            config_name: AIé…ç½®åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é…ç½®
            interactive: æ˜¯å¦äº¤äº’å¼é€‰æ‹©æ¨¡å‹å’Œå‚æ•°
            enable_cache: æ˜¯å¦å¯ç”¨æ„å›¾åˆ†æç¼“å­˜
            enable_async: æ˜¯å¦å¯ç”¨å¼‚æ­¥å¤„ç†èƒ½åŠ›
        """
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨æ± é¿å…é‡å¤åˆå§‹åŒ–
        self.config_pool = ConfigManagerPool()
        self.config_manager = self.config_pool.get_config_manager()
        self.ai_client = self.config_pool.get_ai_client()
        
        self.config_name = config_name
        self.interactive = interactive
        self.enable_cache = enable_cache
        self.enable_async = enable_async
        self.model_id = None
        self.model_parameters = {
            "temperature": 0.1, 
            "stream": True,  # å¯ç”¨æµå¼è¾“å‡º
            "max_tokens": None  # ä¸é™åˆ¶tokenæ•°é‡
        }
        
        # åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜
        self.analysis_cache = IntentAnalysisCache(cache_size=500, ttl=3600) if enable_cache else None
        
        # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
        self.prompts_manager = PromptsManager()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_analyses': 0,
            'cache_hits': 0,
            'ai_calls': 0,
            'total_latency': 0.0,
            'errors': 0
        }
        
        # çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
        self.thread_pool = ThreadPoolExecutor(max_workers=4) if enable_async else None
        
        # é€‰æ‹©AIé…ç½®
        self.config = self._select_config()
        if self.config:
            self.adapter = self.ai_client.create_adapter(self.config)
            
            if self.interactive:
                # äº¤äº’å¼é€‰æ‹©æ¨¡å‹å’Œå‚æ•°ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
                self._interactive_setup_with_cache()
            else:
                # éäº¤äº’æ¨¡å¼ï¼šæ€»æ˜¯è¯¢é—®æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ¨¡å‹
                self._non_interactive_setup_with_cache()
        else:
            raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„AIé…ç½®")
    
    def _non_interactive_setup_with_cache(self):
        """éäº¤äº’æ¨¡å¼ä¸‹çš„ç¼“å­˜é…ç½®å¤„ç†"""
        print("\n[AI] AIæ„å›¾åˆ†æå™¨è®¾ç½®")
        print("=" * 30)
        
        # å°è¯•åŠ è½½ç¼“å­˜é…ç½®
        cached_config = self._load_cached_config()
        
        if cached_config:
            print(f"[OK] å‘ç°ä¸Šæ¬¡é…ç½®:")
            print(f"   é…ç½®: {cached_config.config_name}")
            print(f"   æ¨¡å‹: {cached_config.model_id}")
            print(f"   å‚æ•°: temperature={cached_config.parameters.get('temperature', 0.1)}, max_tokens={cached_config.parameters.get('max_tokens', 'None')}")
            
            # éäº¤äº’æ¨¡å¼ä¸‹è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜é…ç½®
            self.model_id = cached_config.model_id
            self.model_parameters.update(cached_config.parameters)
            print(f"[OK] è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜é…ç½®: {self.model_id}")
            print("ä¿æŒå½“å‰å‚æ•°é…ç½®")
            return
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œè·å–æ–°æ¨¡å‹
        print("[FIND] ä»ç«¯ç‚¹è·å–å¯ç”¨æ¨¡å‹...")
        self.model_id = self._get_default_model()
        if self.model_id:
            print(f"[OK] è·å–åˆ°æ¨¡å‹: {self.model_id}")
            print("ä½¿ç”¨é»˜è®¤å‚æ•°")
            
            # ä¿å­˜æ–°é…ç½®åˆ°ç¼“å­˜
            new_config = AIModelConfig(
                config_name=self.config.name,
                model_id=self.model_id,
                parameters={**self.model_parameters, 'stream': True}  # ç¡®ä¿æµå¼è¾“å‡º
            )
            self._save_config_cache(new_config)
        else:
            print("[FAIL] æ— æ³•è·å–å¯ç”¨æ¨¡å‹")
            raise RuntimeError("æ— æ³•è·å–å¯ç”¨çš„AIæ¨¡å‹")
    
    def _load_cached_config(self) -> Optional[AIModelConfig]:
        """åŠ è½½ç¼“å­˜çš„AIé…ç½®"""
        if os.path.exists(self.CONFIG_CACHE_FILE):
            try:
                with open(self.CONFIG_CACHE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('config_name') == self.config.name:
                        return AIModelConfig.from_dict(data)
            except Exception as e:
                print(f"åŠ è½½é…ç½®ç¼“å­˜å¤±è´¥: {e}")
        return None
    
    def _save_config_cache(self, model_config: AIModelConfig):
        """ä¿å­˜AIé…ç½®åˆ°ç¼“å­˜"""
        try:
            # ç¡®ä¿streamå‚æ•°å§‹ç»ˆä¸ºTrue
            model_config.parameters['stream'] = True
            with open(self.CONFIG_CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(model_config.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜é…ç½®ç¼“å­˜å¤±è´¥: {e}")
    
    def _interactive_setup_with_cache(self):
        """æ”¯æŒç¼“å­˜çš„äº¤äº’å¼è®¾ç½®æ¨¡å‹å’Œå‚æ•°"""
        print("\n[AI] AIæ„å›¾åˆ†æå™¨è®¾ç½®")
        print("=" * 30)
        
        # å°è¯•åŠ è½½ç¼“å­˜é…ç½®
        cached_config = self._load_cached_config()
        
        if cached_config:
            print(f"[OK] å‘ç°ä¸Šæ¬¡é…ç½®:")
            print(f"   é…ç½®: {cached_config.config_name}")
            print(f"   æ¨¡å‹: {cached_config.model_id}")
            print(f"   å‚æ•°: temperature={cached_config.parameters.get('temperature', 0.1)}, "
                  f"max_tokens={cached_config.parameters.get('max_tokens', 'None')}")
            
            use_cached = input("\næ˜¯å¦ä½¿ç”¨ä¸Šæ¬¡çš„é…ç½®? (y/n) [y]: ").strip().lower()
            
            if use_cached in ['', 'y', 'yes']:
                self.model_id = cached_config.model_id
                self.model_parameters = cached_config.parameters
                print("[OK] ä½¿ç”¨ç¼“å­˜é…ç½®")
                
                # å³ä½¿ä½¿ç”¨ç¼“å­˜é…ç½®ï¼Œä¹Ÿè¯¢é—®æ˜¯å¦è¦é‡æ–°è°ƒèŠ‚å‚æ•°
                print(f"\nå½“å‰æ¨¡å‹: {self.model_id}")
                config_params = input("æ˜¯å¦é‡æ–°é…ç½®æ¨¡å‹å‚æ•°? (y/n) [n]: ").strip().lower()
                
                if config_params in ['y', 'yes']:
                    # é‡æ–°é…ç½®å‚æ•°
                    self.model_parameters = self.ai_client.configure_parameters(
                        self.adapter, self.model_id
                    )
                    # ä¿å­˜æ–°é…ç½®åˆ°ç¼“å­˜
                    new_config = AIModelConfig(
                        config_name=self.config.name,
                        model_id=self.model_id,
                        parameters={**self.model_parameters, 'stream': True}  # ç¡®ä¿æµå¼è¾“å‡º
                    )
                    self._save_config_cache(new_config)
                    print("[SAVE] æ–°å‚æ•°é…ç½®å·²ä¿å­˜")
                else:
                    print("ä¿æŒå½“å‰å‚æ•°é…ç½®")
                
                return
            else:
                print("[CONFIG]  é‡æ–°é…ç½®...")
        
        # è¿›è¡Œæ–°çš„é…ç½®
        self._perform_interactive_setup()
        
        # ä¿å­˜æ–°é…ç½®åˆ°ç¼“å­˜
        new_config = AIModelConfig(
            config_name=self.config.name,
            model_id=self.model_id,
            parameters={**self.model_parameters, 'stream': True}  # ç¡®ä¿æµå¼è¾“å‡º
        )
        self._save_config_cache(new_config)
        print("[SAVE] é…ç½®å·²ä¿å­˜ï¼Œä¸‹æ¬¡å°†è‡ªåŠ¨ä½¿ç”¨")
    
    def _perform_interactive_setup(self):
        """æ‰§è¡Œäº¤äº’å¼é…ç½®è¿‡ç¨‹"""
        # é€‰æ‹©æ¨¡å‹
        self.model_id = self._get_default_model()
        if not self.model_id:
            print("[WARN]  ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            self.model_id = self._get_default_model()
            if not self.model_id:
                raise RuntimeError("æ— æ³•è·å–å¯ç”¨æ¨¡å‹")
        
        # è¯¢é—®æ˜¯å¦é…ç½®å‚æ•°
        print(f"\nå½“å‰æ¨¡å‹: {self.model_id}")
        config_params = input("æ˜¯å¦é…ç½®æ¨¡å‹å‚æ•°? (y/n) [n]: ").strip().lower()
        
        if config_params in ['y', 'yes']:
            # é…ç½®å‚æ•°
            self.model_parameters = self.ai_client.configure_parameters(
                self.adapter, self.model_id
            )
        else:
            print("ä½¿ç”¨é»˜è®¤å‚æ•°")
        
        print("[OK] AIåˆ†æå™¨è®¾ç½®å®Œæˆ\n")
    
    def clear_config_cache(self):
        """æ¸…é™¤é…ç½®ç¼“å­˜"""
        try:
            if os.path.exists(self.CONFIG_CACHE_FILE):
                os.remove(self.CONFIG_CACHE_FILE)
                print("[OK] é…ç½®ç¼“å­˜å·²æ¸…é™¤")
            else:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°é…ç½®ç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            print(f"[FAIL] æ¸…é™¤é…ç½®ç¼“å­˜å¤±è´¥: {e}")
    
    @classmethod
    def show_cached_config(cls):
        """æ˜¾ç¤ºå½“å‰ç¼“å­˜çš„é…ç½®"""
        if os.path.exists(cls.CONFIG_CACHE_FILE):
            try:
                with open(cls.CONFIG_CACHE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"\n[LIST] å½“å‰ç¼“å­˜é…ç½®:")
                    print(f"   é…ç½®: {data.get('config_name', 'unknown')}")
                    print(f"   æ¨¡å‹: {data.get('model_id', 'unknown')}")
                    params = data.get('parameters', {})
                    print(f"   å‚æ•°: temperature={params.get('temperature', 'unknown')}, "
                          f"max_tokens={params.get('max_tokens', 'unknown')}")
            except Exception as e:
                print(f"[FAIL] è¯»å–ç¼“å­˜é…ç½®å¤±è´¥: {e}")
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°é…ç½®ç¼“å­˜æ–‡ä»¶")
    
    def _select_config(self):
        """é€‰æ‹©AIé…ç½®"""
        configs = self.config_manager.list_configs()
        
        if not configs:
            return None
        
        if self.config_name:
            return self.config_manager.get_config(self.config_name)
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é…ç½®
            return self.config_manager.get_config(configs[0])
    
    def _get_default_model(self):
        """ä»ç«¯ç‚¹è·å–æ¨¡å‹å¹¶è®©ç”¨æˆ·é€‰æ‹©"""
        try:
            models = self.adapter.get_available_models()
            if not models:
                print("[FAIL] ç«¯ç‚¹æœªè¿”å›å¯ç”¨æ¨¡å‹")
                return None
            
            # æŸ¥æ‰¾ gemini-2.5-pro æ¨¡å‹çš„ç´¢å¼•
            preferred_index = None
            for i, model in enumerate(models):
                if "gemini-2.5-pro" in model.id.lower():
                    preferred_index = i + 1  # æ˜¾ç¤ºçš„åºå·æ˜¯ä»1å¼€å§‹çš„
                    break
            
            print(f"\n{'='*50}")
            print(f"[FIND] ä»ç«¯ç‚¹è·å–åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹:")
            print('='*50)
            for i, model in enumerate(models, 1):
                prefix = "ğŸŒŸ " if preferred_index == i else "  "
                print(f"{prefix}{i}. {model.id}")
            print('='*50)
            
            # è®¾ç½®é»˜è®¤é€‰é¡¹æç¤º
            default_choice = f"[{preferred_index}]" if preferred_index else "[1]"
            default_index = preferred_index - 1 if preferred_index else 0
            
            while True:
                try:
                    choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}) {default_choice}: ").strip()
                    if not choice:
                        selected_index = default_index  # é»˜è®¤é€‰æ‹©
                    else:
                        selected_index = int(choice) - 1
                    
                    if 0 <= selected_index < len(models):
                        selected_model = models[selected_index]
                        print(f"[OK] å·²é€‰æ‹©æ¨¡å‹: {selected_model.id}")
                        return selected_model.id
                    else:
                        print(f"è¯·è¾“å…¥ 1-{len(models)} ä¹‹é—´çš„æ•°å­—")
                except (ValueError, EOFError):
                    # å¦‚æœæ˜¯æ— è¾“å…¥ç¯å¢ƒæˆ–è¾“å…¥é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤é€‰æ‹©
                    selected_model = models[default_index]
                    print(f"[OK] è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ¨¡å‹: {selected_model.id}")
                    return selected_model.id
                    
        except Exception as e:
            print(f"[FAIL] è·å–ç«¯ç‚¹æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def analyze_intent(self, user_input: str) -> SearchCriteria:
        """
        åˆ†æç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆæœç´¢æ¡ä»¶ - ä¼˜åŒ–ç‰ˆ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        Returns:
            SearchCriteria: è§£æåçš„æœç´¢æ¡ä»¶
        """
        start_time = time.time()
        self.performance_stats['total_analyses'] += 1
        
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and self.analysis_cache:
            cached_result = self.analysis_cache.get(user_input, self.model_id, self.model_parameters)
            if cached_result:
                self.performance_stats['cache_hits'] += 1
                return cached_result
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_analysis_prompt(user_input)
        
        # æ„å»ºæ¶ˆæ¯
        messages = [ChatMessage(role="user", content=prompt)]
        
        # è°ƒç”¨AIåˆ†æ
        try:
            response = self.adapter.send_message(
                messages, 
                self.model_id, 
                self.model_parameters
            )
            
            self.performance_stats['ai_calls'] += 1
            
            # ç›´æ¥å¤„ç†å“åº”è§£æï¼Œä¸ä¾èµ–format_responseæ–¹æ³•
            ai_response = self._extract_response_content(response)
            
            # å¦‚æœå“åº”ä¸ºç©ºï¼Œä½¿ç”¨åŸºç¡€ç­–ç•¥
            if not ai_response or ai_response.strip() == "":
                print(f"è§£æAIå“åº”å¤±è´¥: æœåŠ¡å™¨è¿”å›ç©ºå“åº”")
                print(f"AIå“åº”å†…å®¹: (ç©º)")
                return SearchCriteria(query=user_input)
            
            criteria = self._parse_ai_response_with_validation(ai_response, user_input)
            
            # ç¼“å­˜ç»“æœ
            if self.enable_cache and self.analysis_cache and not criteria.query == user_input:
                self.analysis_cache.put(user_input, self.model_id, self.model_parameters, criteria)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            latency = time.time() - start_time
            self.performance_stats['total_latency'] += latency
            
            return criteria
            
        except Exception as e:
            self.performance_stats['errors'] += 1
            print(f"AIåˆ†æå¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æœç´¢æ¡ä»¶
            return SearchCriteria(query=user_input)
    
    async def analyze_intent_async(self, user_input: str) -> SearchCriteria:
        """
        å¼‚æ­¥åˆ†æç”¨æˆ·æ„å›¾
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            
        Returns:
            SearchCriteria: è§£æåçš„æœç´¢æ¡ä»¶
        """
        if not self.enable_async or not self.thread_pool:
            # å¦‚æœæœªå¯ç”¨å¼‚æ­¥ï¼Œç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
            return self.analyze_intent(user_input)
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, self.analyze_intent, user_input)
    
    def analyze_batch_intents(self, user_inputs: List[str]) -> List[SearchCriteria]:
        """
        æ‰¹é‡åˆ†æç”¨æˆ·æ„å›¾
        
        Args:
            user_inputs: ç”¨æˆ·è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[SearchCriteria]: è§£æåçš„æœç´¢æ¡ä»¶åˆ—è¡¨
        """
        if not self.enable_async or not self.thread_pool:
            # åŒæ­¥æ‰¹é‡å¤„ç†
            return [self.analyze_intent(input_text) for input_text in user_inputs]
        
        # å¼‚æ­¥æ‰¹é‡å¤„ç†
        futures = []
        for input_text in user_inputs:
            future = self.thread_pool.submit(self.analyze_intent, input_text)
            futures.append(future)
        
        results = []
        for future in futures:
            try:
                result = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                results.append(result)
            except Exception as e:
                print(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")
                # è¿”å›åŸºç¡€æœç´¢æ¡ä»¶
                results.append(SearchCriteria(query=input_text))
        
        return results
    
    def _build_analysis_prompt(self, user_input: str) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½æç¤ºè¯
        try:
            prompt_template = self.prompts_manager.get_intent_analysis_prompt(user_input)
            if prompt_template and len(prompt_template.strip()) > 100:  # ç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–å¤ªçŸ­çš„æ¨¡æ¿
                return prompt_template
        except Exception as e:
            print(f"[WARN]  ä½¿ç”¨é…ç½®æ–‡ä»¶æç¤ºè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
        
        # å›é€€åˆ°é»˜è®¤æç¤ºè¯
        return self._build_default_analysis_prompt(user_input)
    
    def _build_default_analysis_prompt(self, user_input: str) -> str:
        """æ„å»ºé»˜è®¤åˆ†ææç¤ºè¯ï¼ˆå…¼å®¹æ€§ä¿è¯ï¼‰"""
        from datetime import datetime
        current_date = datetime.now()
        current_year = current_date.year
        current_month = current_date.month
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ–‡çŒ®æ£€ç´¢ä¸“å®¶ï¼Œéœ€è¦åˆ†æç”¨æˆ·çš„æ£€ç´¢éœ€æ±‚å¹¶ç”Ÿæˆç›¸åº”çš„PubMedæ£€ç´¢è¯å’Œç­›é€‰æ¡ä»¶ã€‚

å½“å‰æ—¥æœŸ: {current_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} (ç¬¬{current_year}å¹´)
ç”¨æˆ·è¾“å…¥: "{user_input}"

è¯·åˆ†æç”¨æˆ·çš„æ„å›¾å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

1. **query**: PubMedæ£€ç´¢è¯ï¼ˆä½¿ç”¨å¸ƒå°”æ“ä½œç¬¦ANDã€ORã€NOTï¼Œä½¿ç”¨MeSHè¯æ±‡ï¼‰
2. **year_start**: èµ·å§‹å¹´ä»½ï¼ˆæ•´æ•°ï¼Œå¦‚æœç”¨æˆ·æåˆ°äº†å¹´ä»½é™åˆ¶ï¼‰
3. **year_end**: ç»“æŸå¹´ä»½ï¼ˆæ•´æ•°ï¼Œå¦‚æœç”¨æˆ·æåˆ°äº†å¹´ä»½é™åˆ¶ï¼‰  
4. **min_if**: æœ€å°å½±å“å› å­ï¼ˆæµ®ç‚¹æ•°ï¼Œå¦‚æœç”¨æˆ·æåˆ°å½±å“å› å­è¦æ±‚ï¼‰
5. **max_if**: æœ€å¤§å½±å“å› å­ï¼ˆæµ®ç‚¹æ•°ï¼Œå¦‚æœç”¨æˆ·æåˆ°å½±å“å› å­è¦æ±‚ï¼‰
6. **cas_zones**: ä¸­ç§‘é™¢åˆ†åŒºé™åˆ¶ï¼ˆæ•´æ•°åˆ—è¡¨ï¼Œ1-4åˆ†åŒºï¼Œå¦‚[1,2]è¡¨ç¤º1åŒºå’Œ2åŒºï¼‰
7. **jcr_quartiles**: JCRåˆ†åŒºé™åˆ¶ï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¦‚["Q1","Q2"]ï¼‰
8. **keywords**: å…³é”®è¯è¿‡æ»¤åˆ—è¡¨ï¼ˆä»ç”¨æˆ·è¾“å…¥ä¸­æå–çš„é‡è¦å…³é”®è¯ï¼‰

åˆ†æè§„åˆ™ï¼š
- è¯†åˆ«ç–¾ç—…åç§°ã€æ²»ç–—æ–¹æ³•ã€è¯ç‰©åç§°ç­‰åŒ»å­¦æ¦‚å¿µ
- å°†ä¸­æ–‡åŒ»å­¦æœ¯è¯­è½¬æ¢ä¸ºè‹±æ–‡å’ŒMeSHæœ¯è¯­
- è‡ªåŠ¨è¡¥å……ç›¸å…³çš„åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­
- **é‡è¦ï¼šåŸºäºå½“å‰æ—¥æœŸ({current_year}å¹´{current_month}æœˆ)ç²¾ç¡®è®¡ç®—å¹´ä»½é™åˆ¶**
  - "è¿‘å¹´æ¥"æˆ–"æœ€è¿‘"ï¼š{current_year-2}-{current_year}å¹´
  - "è¿‘3å¹´"ï¼š{current_year-2}-{current_year}å¹´
  - "è¿‘5å¹´"ï¼š{current_year-4}-{current_year}å¹´
  - "è¿‘10å¹´"ï¼š{current_year-9}-{current_year}å¹´
  - "æœ€è¿‘å‡ å¹´"ï¼š{current_year-3}-{current_year}å¹´
  - "è¿‡å»5å¹´"ï¼š{current_year-4}-{current_year}å¹´
  - "2020å¹´ä»¥æ¥"ï¼š2020-{current_year}å¹´
  - "ç–«æƒ…æœŸé—´"æˆ–"COVIDæœŸé—´"ï¼š2020-{current_year}å¹´
- å½±å“å› å­ï¼šé«˜å½±å“å› å­=5.0ä»¥ä¸Šï¼Œé¡¶çº§æœŸåˆŠ=10.0ä»¥ä¸Š
- **é‡è¦åˆ†åŒºè§„åˆ™ï¼š**
  - ä»…å½“ç”¨æˆ·æ˜ç¡®æåˆ°"ä¸­ç§‘é™¢åˆ†åŒº"æˆ–"CASåˆ†åŒº"æ—¶ï¼Œæ‰è¾“å‡ºcas_zones
  - ä»…å½“ç”¨æˆ·æ˜ç¡®æåˆ°"JCRåˆ†åŒº"æˆ–"JCR quartile"æ—¶ï¼Œæ‰è¾“å‡ºjcr_quartiles
  - ç”¨æˆ·ä»…è¯´"é«˜å½±å“å› å­"æˆ–"é¡¶çº§æœŸåˆŠ"æ—¶ï¼Œåªè¾“å‡ºmin_ifï¼Œä¸è¦è‡ªåŠ¨æ·»åŠ åˆ†åŒºé™åˆ¶
  - ç”¨æˆ·æ˜ç¡®è¯´"ä¸­ç§‘é™¢1åŒº"æ—¶ï¼Œåªè¾“å‡ºcas_zones: [1]ï¼Œä¸è¦æ·»åŠ jcr_quartiles
  - ç”¨æˆ·æ˜ç¡®è¯´"JCR Q1"æ—¶ï¼Œåªè¾“å‡ºjcr_quartiles: ["Q1"]ï¼Œä¸è¦æ·»åŠ cas_zones

ç¤ºä¾‹1 - ä»…å½±å“å› å­è¦æ±‚ï¼š
ç”¨æˆ·è¾“å…¥ï¼š"ç³–å°¿ç—…æ²»ç–—çš„æœ€æ–°ç ”ç©¶ï¼Œè¦æ±‚æ˜¯è¿‘5å¹´çš„é«˜å½±å“å› å­æœŸåˆŠæ–‡çŒ®"
åŸºäºå½“å‰æ—¥æœŸ({current_year}å¹´)ï¼Œ"è¿‘5å¹´"åº”è§£æä¸º{current_year-4}-{current_year}å¹´
è¾“å‡ºï¼š
```json
{{
  "query": "(diabetes mellitus[MeSH Terms] OR diabetes[Title/Abstract]) AND (treatment[MeSH Terms] OR therapy[Title/Abstract] OR therapeutic[Title/Abstract])",
  "year_start": {current_year-4},
  "year_end": {current_year},
  "min_if": 5.0,
  "keywords": ["diabetes", "treatment", "therapy", "diabetes mellitus"]
}}
```

ç¤ºä¾‹2 - ä»…ä¸­ç§‘é™¢åˆ†åŒºè¦æ±‚ï¼š
ç”¨æˆ·è¾“å…¥ï¼š"é«˜è¡€å‹æ²»ç–—ï¼Œä¸­ç§‘é™¢1åŒºå’Œ2åŒºæœŸåˆŠ"
è¾“å‡ºï¼š
```json
{{
  "query": "(hypertension[MeSH Terms] OR high blood pressure[Title/Abstract]) AND (treatment[MeSH Terms] OR therapy[Title/Abstract])",
  "cas_zones": [1, 2],
  "keywords": ["hypertension", "treatment", "high blood pressure"]
}}
```

ç¤ºä¾‹3 - ä»…JCRåˆ†åŒºè¦æ±‚ï¼š
ç”¨æˆ·è¾“å…¥ï¼š"ç™Œç—‡å…ç–«æ²»ç–—ç ”ç©¶ï¼Œè¦æ±‚JCR Q1æœŸåˆŠ"
è¾“å‡ºï¼š
```json
{{
  "query": "(cancer[MeSH Terms] OR neoplasms[MeSH Terms]) AND (immunotherapy[MeSH Terms] OR immune therapy[Title/Abstract])",
  "jcr_quartiles": ["Q1"],
  "keywords": ["cancer", "immunotherapy", "immune therapy"]
}}
```

è¯·å¯¹ä¸Šè¿°ç”¨æˆ·è¾“å…¥è¿›è¡Œåˆ†æå¹¶è¾“å‡ºJSONæ ¼å¼ç»“æœï¼Œç¡®ä¿å¹´ä»½è®¡ç®—å‡†ç¡®ï¼š
"""
        return prompt
    
    def _extract_response_content(self, response: Dict[str, Any]) -> str:
        """ç›´æ¥ä»å“åº”ä¸­æå–å†…å®¹ï¼Œä¸ä¾èµ–å¤–éƒ¨format_response"""
        if 'error' in response:
            return f"é”™è¯¯: {response['error']}"
        
        try:
            if self.adapter.config.api_type.lower() == 'openai':
                choices = response.get('choices', [])
                if choices:
                    content = choices[0].get('message', {}).get('content', '')
                    if isinstance(content, list):
                        content = ' '.join(str(item) for item in content)
                    elif not isinstance(content, str):
                        content = str(content)
                    return content.strip()
            
            elif self.adapter.config.api_type.lower() == 'gemini':
                candidates = response.get('candidates', [])
                if candidates:
                    content = candidates[0].get('content', {})
                    parts = content.get('parts', [])
                    if parts:
                        text = parts[0].get('text', '')
                        if isinstance(text, list):
                            text = ' '.join(str(item) for item in text)
                        elif not isinstance(text, str):
                            text = str(text)
                        return text.strip()
            
        except (KeyError, IndexError, TypeError) as e:
            print(f"å“åº”å†…å®¹æå–å¤±è´¥: {e}")
            return ""
        
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""

    def _parse_ai_response_with_validation(self, ai_response: str, original_input: str) -> SearchCriteria:
        """è§£æAIå“åº” - å¢å¼ºéªŒè¯å’Œé”™è¯¯æ¢å¤"""
        try:
            # æå–JSONéƒ¨åˆ† - ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = ai_response.strip()
            
            # æ¸…ç†JSONå­—ç¬¦ä¸²
            json_str = self._clean_json_string(json_str)
            
            # è§£æJSON
            data = json.loads(json_str)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            validated_data = self._validate_search_criteria(data, original_input)
            
            return SearchCriteria(
                query=validated_data.get('query', original_input),
                year_start=validated_data.get('year_start'),
                year_end=validated_data.get('year_end'),
                min_if=validated_data.get('min_if'),
                max_if=validated_data.get('max_if'),
                cas_zones=validated_data.get('cas_zones', []),
                jcr_quartiles=validated_data.get('jcr_quartiles', []),
                keywords=validated_data.get('keywords', [])
            )
            
        except (json.JSONDecodeError, KeyError) as e:
            print(f"è§£æAIå“åº”å¤±è´¥: {e}")
            print(f"AIå“åº”å†…å®¹: {ai_response[:500]}...")
            
            # å°è¯•ä½¿ç”¨æ›´å®½æ¾çš„è§£æç­–ç•¥
            return self._fallback_parse_response(ai_response, original_input)
    
    def _clean_json_string(self, json_str: str) -> str:
        """æ¸…ç†JSONå­—ç¬¦ä¸²"""
        # ç§»é™¤æ³¨é‡Š
        json_str = re.sub(r'//.*?\n', '\n', json_str)
        json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
        
        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        json_str = json_str.replace("'", '"')  # å•å¼•å·è½¬åŒå¼•å·
        json_str = re.sub(r',\s*}', '}', json_str)  # ç§»é™¤å°¾éšé€—å·
        json_str = re.sub(r',\s*]', ']', json_str)  # ç§»é™¤æ•°ç»„å°¾éšé€—å·
        
        return json_str.strip()
    
    def _validate_search_criteria(self, data: Dict, original_input: str) -> Dict:
        """éªŒè¯å’Œä¿®å¤æœç´¢æ¡ä»¶æ•°æ®"""
        validated = {}
        
        # éªŒè¯queryå­—æ®µ
        query = data.get('query', '').strip()
        if not query or len(query) < 3:
            # å¦‚æœqueryä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥
            validated['query'] = original_input
        else:
            validated['query'] = query
        
        # éªŒè¯å¹´ä»½èŒƒå›´
        year_start = data.get('year_start')
        year_end = data.get('year_end')
        current_year = datetime.now().year
        
        if year_start and year_end:
            if year_start > year_end:
                # äº¤æ¢èµ·å§‹å’Œç»“æŸå¹´ä»½
                year_start, year_end = year_end, year_start
            if year_end > current_year + 1:
                year_end = current_year
        
        validated['year_start'] = year_start
        validated['year_end'] = year_end
        
        # éªŒè¯å½±å“å› å­
        min_if = data.get('min_if')
        max_if = data.get('max_if')
        
        if min_if and max_if:
            if min_if > max_if:
                min_if, max_if = max_if, min_if
            if min_if < 0:
                min_if = 0
            if max_if > 100:
                max_if = 100
        
        validated['min_if'] = min_if
        validated['max_if'] = max_if
        
        # éªŒè¯åˆ†åŒºä¿¡æ¯
        cas_zones = data.get('cas_zones', [])
        if isinstance(cas_zones, list):
            validated['cas_zones'] = [zone for zone in cas_zones if 1 <= zone <= 4]
        else:
            validated['cas_zones'] = []
        
        jcr_quartiles = data.get('jcr_quartiles', [])
        if isinstance(jcr_quartiles, list):
            valid_quartiles = ['Q1', 'Q2', 'Q3', 'Q4']
            validated['jcr_quartiles'] = [q for q in jcr_quartiles if q in valid_quartiles]
        else:
            validated['jcr_quartiles'] = []
        
        # éªŒè¯å…³é”®è¯
        keywords = data.get('keywords', [])
        if isinstance(keywords, list):
            validated['keywords'] = [kw.strip() for kw in keywords if kw.strip()]
        else:
            validated['keywords'] = []
        
        return validated
    
    def _fallback_parse_response(self, ai_response: str, original_input: str) -> SearchCriteria:
        """å›é€€è§£æç­–ç•¥"""
        # å°è¯•æå–queryå­—æ®µ
        query_match = re.search(r'"query"\s*:\s*"([^"]+)"', ai_response)
        if query_match:
            query = query_match.group(1)
        else:
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–æŸ¥è¯¢è¯
            query = self._extract_basic_query(ai_response)
        
        # å°è¯•æå–å¹´ä»½ä¿¡æ¯
        year_start = None
        year_end = None
        year_matches = re.findall(r'(20\d{2})', ai_response)
        if year_matches:
            years = sorted(set(int(y) for y in year_matches))
            if len(years) >= 2:
                year_start = min(years)
                year_end = max(years)
        
        return SearchCriteria(
            query=query or original_input,
            year_start=year_start,
            year_end=year_end
        )
    
    def _extract_basic_query(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–åŸºç¡€æŸ¥è¯¢è¯"""
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        lines = response.split('\n')
        for line in lines:
            if 'query' in line.lower() and ':' in line:
                parts = line.split(':', 1)
                if len(parts) > 1:
                    return parts[1].strip().strip('"')
        
        return response[:100]  # è¿”å›å‰100å­—ç¬¦ä½œä¸ºæŸ¥è¯¢è¯
    
    def build_pubmed_query(self, criteria: SearchCriteria) -> str:
        """æ„å»ºå®Œæ•´çš„PubMedæŸ¥è¯¢å­—ç¬¦ä¸²"""
        query_parts = [criteria.query]
        
        # æ·»åŠ å¹´ä»½é™åˆ¶
        if criteria.year_start or criteria.year_end:
            year_filter = ""
            if criteria.year_start and criteria.year_end:
                year_filter = f"(\"{criteria.year_start}\"[Date - Publication] : \"{criteria.year_end}\"[Date - Publication])"
            elif criteria.year_start:
                year_filter = f"\"{criteria.year_start}\"[Date - Publication] : 3000[Date - Publication]"
            elif criteria.year_end:
                year_filter = f"1800[Date - Publication] : \"{criteria.year_end}\"[Date - Publication]"
            
            if year_filter:
                query_parts.append(year_filter)
        
        # ç»„åˆæ‰€æœ‰æŸ¥è¯¢éƒ¨åˆ†
        final_query = " AND ".join(f"({part})" for part in query_parts if part.strip())
        
        return final_query
    
    def print_analysis_result(self, criteria: SearchCriteria):
        """æ‰“å°åˆ†æç»“æœ"""
        print("\n=== æ„å›¾åˆ†æç»“æœ ===")
        print(f"PubMedæ£€ç´¢è¯: {criteria.query}")
        
        if criteria.year_start or criteria.year_end:
            year_range = f"{criteria.year_start or 'ä¸é™'} - {criteria.year_end or 'ä¸é™'}"
            print(f"å¹´ä»½é™åˆ¶: {year_range}")
        
        if criteria.min_if or criteria.max_if:
            if_range = f"{criteria.min_if or 'ä¸é™'} - {criteria.max_if or 'ä¸é™'}"
            print(f"å½±å“å› å­èŒƒå›´: {if_range}")
        
        # åˆ†åˆ«æ˜¾ç¤ºåˆ†åŒºä¿¡æ¯ï¼Œé¿å…æ··æ·†
        if criteria.cas_zones:
            print(f"ä¸­ç§‘é™¢åˆ†åŒºé™åˆ¶: {', '.join(map(str, criteria.cas_zones))}åŒº")
        
        if criteria.jcr_quartiles:
            print(f"JCRåˆ†åŒºé™åˆ¶: {', '.join(criteria.jcr_quartiles)}")
        
        if criteria.keywords:
            print(f"å…³é”®è¯: {', '.join(criteria.keywords)}")
        
        print(f"å®Œæ•´æ£€ç´¢è¯: {self.build_pubmed_query(criteria)}")
        print("=" * 40)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        total_analyses = self.performance_stats['total_analyses']
        
        report = {
            'performance_stats': self.performance_stats.copy(),
            'cache_enabled': self.enable_cache,
            'async_enabled': self.enable_async,
            'cache_stats': self.analysis_cache.get_stats() if self.analysis_cache else {},
            'analysis_rate': 0,
            'cache_hit_rate': 0,
            'error_rate': 0,
            'average_latency': 0
        }
        
        if total_analyses > 0:
            report['cache_hit_rate'] = (self.performance_stats['cache_hits'] / total_analyses) * 100
            report['error_rate'] = (self.performance_stats['errors'] / total_analyses) * 100
            report['average_latency'] = self.performance_stats['total_latency'] / total_analyses
        
        return report
    
    def print_performance_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        report = self.get_performance_report()
        stats = report['performance_stats']
        
        print("\n=== æ„å›¾åˆ†æå™¨æ€§èƒ½æŠ¥å‘Š ===")
        print(f"æ€»åˆ†ææ¬¡æ•°: {stats['total_analyses']}")
        print(f"ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"AIè°ƒç”¨æ¬¡æ•°: {stats['ai_calls']}")
        print(f"é”™è¯¯æ¬¡æ•°: {stats['errors']}")
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {report['cache_hit_rate']:.1f}%")
        print(f"é”™è¯¯ç‡: {report['error_rate']:.1f}%")
        print(f"å¹³å‡å»¶è¿Ÿ: {report['average_latency']:.2f}ç§’")
        
        if report['cache_stats']:
            cache_stats = report['cache_stats']
            print(f"ç¼“å­˜å¤§å°: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}%")
        
        print("=" * 40)
    
    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        if self.analysis_cache:
            self.analysis_cache.clear()
        
        # æ¸…é™¤é…ç½®ç¼“å­˜
        try:
            if os.path.exists(self.CONFIG_CACHE_FILE):
                os.remove(self.CONFIG_CACHE_FILE)
        except Exception as e:
            print(f"æ¸…é™¤é…ç½®ç¼“å­˜å¤±è´¥: {e}")
        
        print("æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")
    
    def optimize_for_batch(self, expected_batch_size: int):
        """ä¸ºæ‰¹é‡å¤„ç†ä¼˜åŒ–ç¼“å­˜å¤§å°"""
        if self.analysis_cache:
            # æ ¹æ®æ‰¹é‡å¤§å°åŠ¨æ€è°ƒæ•´ç¼“å­˜
            new_cache_size = max(500, expected_batch_size * 2)
            self.analysis_cache.cache_size = new_cache_size
            print(f"ç¼“å­˜å¤§å°å·²è°ƒæ•´ä¸º: {new_cache_size}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        if hasattr(self, 'thread_pool') and self.thread_pool:
            self.thread_pool.shutdown(wait=False)


def test_intent_analyzer():
    """æµ‹è¯•æ„å›¾åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    try:
        print("åˆå§‹åŒ–æ„å›¾åˆ†æå™¨...")
        analyzer = IntentAnalyzer(interactive=False, enable_cache=True, enable_async=True)
        
        # æµ‹è¯•ç”¨ä¾‹
        test_inputs = [
            "ç³–å°¿ç—…æ²»ç–—çš„æœ€æ–°ç ”ç©¶ï¼Œè¦æ±‚æ˜¯è¿‘5å¹´çš„é«˜å½±å“å› å­æœŸåˆŠæ–‡çŒ®",
            "æ–°å† è‚ºç‚COVID-19ç–«è‹—æ•ˆæœç ”ç©¶ï¼Œ2020-2023å¹´ï¼Œä¸­ç§‘é™¢1åŒºæœŸåˆŠ",
            "æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œå½±å“å› å­å¤§äº3ï¼ŒJCR Q1-Q2æœŸåˆŠ",
            "é˜¿å°”èŒ¨æµ·é»˜ç—…æ–°è¯ç‰©æ²»ç–—è¿›å±•"
        ]
        
        print("\n=== æµ‹è¯•åŒæ­¥åˆ†æ ===")
        for i, user_input in enumerate(test_inputs, 1):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i}: {user_input}")
            criteria = analyzer.analyze_intent(user_input)
            analyzer.print_analysis_result(criteria)
        
        # æµ‹è¯•ç¼“å­˜æ•ˆæœ
        print("\n=== æµ‹è¯•ç¼“å­˜æ•ˆæœ ===")
        start_time = time.time()
        for i, user_input in enumerate(test_inputs, 1):
            print(f"\nç¼“å­˜æµ‹è¯• {i}: {user_input}")
            criteria = analyzer.analyze_intent(user_input)  # åº”è¯¥å‘½ä¸­ç¼“å­˜
        cache_time = time.time() - start_time
        print(f"ç¼“å­˜åˆ†ææ€»è€—æ—¶: {cache_time:.2f}ç§’")
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        print("\n=== æµ‹è¯•æ‰¹é‡å¤„ç† ===")
        analyzer.optimize_for_batch(len(test_inputs))
        start_time = time.time()
        batch_results = analyzer.analyze_batch_intents(test_inputs)
        batch_time = time.time() - start_time
        print(f"æ‰¹é‡åˆ†ææ€»è€—æ—¶: {batch_time:.2f}ç§’")
        print(f"å¹³å‡æ¯ä¸ªåˆ†æ: {batch_time/len(test_inputs):.2f}ç§’")
        
        # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
        analyzer.print_performance_report()
        
        # æµ‹è¯•å¼‚æ­¥å¤„ç†ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if analyzer.enable_async:
            print("\n=== æµ‹è¯•å¼‚æ­¥å¤„ç† ===")
            import asyncio
            
            async def test_async():
                start_time = time.time()
                tasks = [analyzer.analyze_intent_async(input_text) for input_text in test_inputs]
                results = await asyncio.gather(*tasks)
                async_time = time.time() - start_time
                print(f"å¼‚æ­¥åˆ†ææ€»è€—æ—¶: {async_time:.2f}ç§’")
                return results
            
            asyncio.run(test_async())
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_intent_analyzer()